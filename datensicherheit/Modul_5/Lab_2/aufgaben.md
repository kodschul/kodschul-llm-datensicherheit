# üîπ Lab 2.2 ‚Äì Datenminimierung durch Chunking

## üîç Preview

TN optimieren die **Granularit√§t** der Dokumente, um nur minimal n√∂tige Informationen zu retrieven.

**Wichtig:** Gro√üe Dokumente enthalten oft unn√∂tige Informationen. Durch cleveres Chunking holen wir nur das N√∂tigste!

---

## üß© Situation

**Problem:** Aktuell werden ganze Dokumente retrieved!

```python
# Dokument in VectorDB:
"""
FAQ - Versand

1. Was kostet Standard-Versand? 4,99‚Ç¨
2. Was kostet Express-Versand? 9,99‚Ç¨
3. Lieferzeiten: Standard 3-5 Tage, Express 1-2 Tage
4. Tracking-Nummern erhalten Sie per E-Mail an: max@example.com
5. Versand erfolgt via DHL, Hermes oder DPD
6. Internationale Lieferungen: Kontakt support@firma.de
7. R√ºcksendungen kostenfrei innerhalb 14 Tage
8. R√ºcksendeadresse: Musterfirma GmbH, Lagerstra√üe 10, 12345 Berlin
"""
```

**Frage:** "Was kostet Express-Versand?"

**Problem:**  
‚Üí Das **gesamte** Dokument wird retrieved!  
‚Üí Enth√§lt E-Mail-Adressen (`max@example.com`, `support@firma.de`)  
‚Üí Enth√§lt Adresse (`Lagerstra√üe 10, 12345 Berlin`)  
‚Üí Alles landet im LLM-Kontext!

**Besser:**  
‚Üí Nur Chunk #2: "Was kostet Express-Versand? 9,99‚Ç¨"

---

## üõ†Ô∏è √úbung ‚Äì Chunk-Optimierung

**Aufgabe 1: Aktuelles Chunking analysieren**

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Beispiel-Dokument
full_document = """
FAQ - Versand und R√ºckgabe

VERSAND:
1. Was kostet Standard-Versand? 4,99‚Ç¨
2. Was kostet Express-Versand? 9,99‚Ç¨
3. Lieferzeiten: Standard 3-5 Tage, Express 1-2 Tage
4. Tracking-Nummern per E-Mail an: max.mustermann@example.com
5. Versandpartner: DHL, Hermes, DPD

R√úCKGABE:
6. R√ºcksendungen kostenfrei innerhalb 14 Tage
7. R√ºcksendeadresse: Musterfirma GmbH, Lagerstra√üe 10, 12345 Berlin
8. Kontakt f√ºr R√ºckfragen: support@firma.de, +49 123 456789

ZAHLUNGSARTEN:
9. Kreditkarte, PayPal, √úberweisung
10. IBAN f√ºr R√ºckerstattung: DE89 3704 0044 0532 0130 00
"""

# Verschiedene Chunk-Gr√∂√üen testen
chunk_sizes = [100, 200, 500]

for size in chunk_sizes:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=size,
        chunk_overlap=20,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    
    chunks = splitter.split_text(full_document)
    
    print(f"\n{'='*60}")
    print(f"Chunk-Size: {size} Zeichen")
    print(f"Anzahl Chunks: {len(chunks)}")
    print(f"{'='*60}")
    
    for i, chunk in enumerate(chunks, 1):
        print(f"\nChunk {i}:")
        print(chunk[:100] + "..." if len(chunk) > 100 else chunk)
```

**Analysiere:**
1. Bei welcher Chunk-Size ist die Granularit√§t optimal?
2. Welche PII landet in welchen Chunks?
3. Gibt es Chunks ohne sensible Daten?

---

**Aufgabe 2: Semantisches Chunking**

**Problem mit Character-basierten Chunks:**  
‚Üí Schneiden mitten in S√§tzen ab!  
‚Üí Keine R√ºcksicht auf Semantik!

**L√∂sung: Semantisches Chunking!**

```python
def semantic_chunking(text: str) -> list:
    """
    Chunking basierend auf semantischen Einheiten
    """
    # Strategie 1: Nach Abs√§tzen trennen
    paragraphs = text.split("\n\n")
    
    # Strategie 2: Nach √úberschriften
    sections = []
    current_section = ""
    current_title = ""
    
    for line in text.split("\n"):
        # √úberschrift erkannt (GROSSBUCHSTABEN, endet mit :)
        if line.isupper() and line.endswith(":"):
            if current_section:
                sections.append({
                    "title": current_title,
                    "content": current_section.strip()
                })
            current_title = line.strip(":")
            current_section = ""
        else:
            current_section += line + "\n"
    
    # Letzter Abschnitt
    if current_section:
        sections.append({
            "title": current_title,
            "content": current_section.strip()
        })
    
    return sections


# Test
semantic_chunks = semantic_chunking(full_document)

print("\nSEMANTISCHE CHUNKS:")
print("="*60)

for chunk in semantic_chunks:
    print(f"\n[{chunk['title']}]")
    print(chunk['content'][:200] + "..." if len(chunk['content']) > 200 else chunk['content'])
```

---

**Aufgabe 3: PII-aware Chunking**

**Idee:**  
Chunks, die PII enthalten, **separat markieren**!

```python
from typing import List, Dict

def pii_aware_chunking(text: str) -> List[Dict]:
    """
    Chunking mit PII-Metadaten
    """
    # Semantische Chunks erstellen
    chunks = semantic_chunking(text)
    
    # PII-Analyse pro Chunk
    from presidio_analyzer import AnalyzerEngine
    analyzer = AnalyzerEngine()
    
    enriched_chunks = []
    
    for chunk in chunks:
        content = chunk['content']
        
        # PII erkennen
        pii_results = analyzer.analyze(
            text=content,
            language="de",
            entities=["PERSON", "EMAIL_ADDRESS", "PHONE_NUMBER", "IBAN_CODE", "LOCATION"]
        )
        
        # PII-Typen extrahieren
        pii_types = list(set(res.entity_type for res in pii_results))
        
        # Sensitivity berechnen
        sensitivity = "public"
        
        if any(t in ["IBAN_CODE", "CREDIT_CARD"] for t in pii_types):
            sensitivity = "critical"
        elif any(t in ["EMAIL_ADDRESS", "PHONE_NUMBER"] for t in pii_types):
            sensitivity = "high"
        elif pii_types:
            sensitivity = "medium"
        
        enriched_chunks.append({
            "title": chunk['title'],
            "content": content,
            "has_pii": len(pii_results) > 0,
            "pii_types": pii_types,
            "pii_count": len(pii_results),
            "sensitivity": sensitivity
        })
    
    return enriched_chunks


# Test
pii_chunks = pii_aware_chunking(full_document)

print("\nPII-AWARE CHUNKS:")
print("="*70)

for i, chunk in enumerate(pii_chunks, 1):
    print(f"\nChunk {i}: {chunk['title']}")
    print(f"  Sensitivity: {chunk['sensitivity']}")
    print(f"  Has PII: {chunk['has_pii']}")
    
    if chunk['has_pii']:
        print(f"  PII Types: {', '.join(chunk['pii_types'])}")
        print(f"  PII Count: {chunk['pii_count']}")
```

---

## üß† Aufgabe (Transfer)

**Aufgabe 4: VectorDB mit PII-Metadaten bef√ºllen**

```python
from langchain.schema import Document
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings

def ingest_with_pii_metadata(chunks: List[Dict], vectorstore_path: str):
    """
    Speichert Chunks mit PII-Metadaten in VectorDB
    """
    documents = []
    
    for chunk in chunks:
        doc = Document(
            page_content=chunk['content'],
            metadata={
                "title": chunk['title'],
                "has_pii": chunk['has_pii'],
                "pii_types": ",".join(chunk['pii_types']),  # String, da Chroma kein Array unterst√ºtzt
                "sensitivity": chunk['sensitivity'],
                "pii_count": chunk['pii_count']
            }
        )
        documents.append(doc)
    
    # VectorDB erstellen
    embeddings = OllamaEmbeddings(model="nomic-embed-text")
    
    vectorstore = Chroma.from_documents(
        documents=documents,
        embedding=embeddings,
        persist_directory=vectorstore_path
    )
    
    print(f"\n‚úÖ {len(documents)} Chunks in VectorDB gespeichert")
    print(f"   Davon {sum(1 for c in chunks if c['has_pii'])} mit PII")
    
    return vectorstore


# Chunks ingesten
enriched_chunks = pii_aware_chunking(full_document)
vectorstore = ingest_with_pii_metadata(enriched_chunks, "./db/chroma_pii_aware")
```

---

**Aufgabe 5: Pr√§ferenz f√ºr PII-freie Chunks**

```python
def search_with_pii_preference(
    question: str, 
    vectorstore, 
    prefer_pii_free: bool = True,
    k: int = 3
):
    """
    Sucht Chunks, bevorzugt aber PII-freie
    """
    # Schritt 1: Alle relevanten Chunks holen (k*2 f√ºr Auswahl)
    all_results = vectorstore.similarity_search_with_score(
        question, 
        k=k*2
    )
    
    # Schritt 2: Nach PII sortieren
    pii_free = []
    with_pii = []
    
    for doc, score in all_results:
        if not doc.metadata.get("has_pii", False):
            pii_free.append((doc, score))
        else:
            with_pii.append((doc, score))
    
    # Schritt 3: Pr√§ferenz anwenden
    if prefer_pii_free and pii_free:
        # Zuerst PII-freie Chunks, dann mit PII falls n√∂tig
        selected = pii_free[:k]
        
        if len(selected) < k:
            selected += with_pii[:k - len(selected)]
    else:
        # Keine Pr√§ferenz: Nur nach Relevanz
        selected = all_results[:k]
    
    print(f"\nGefunden: {len(pii_free)} PII-freie, {len(with_pii)} mit PII")
    print(f"Ausgew√§hlt: {len([d for d, s in selected if not d.metadata.get('has_pii')])} PII-freie")
    
    return [doc for doc, score in selected]


# Test
question = "Was kostet der Versand?"
results = search_with_pii_preference(question, vectorstore, prefer_pii_free=True, k=2)

print(f"\nERGEBNISSE f√ºr: '{question}'")
print("="*70)

for i, doc in enumerate(results, 1):
    print(f"\nChunk {i}: {doc.metadata['title']}")
    print(f"  Sensitivity: {doc.metadata['sensitivity']}")
    print(f"  Content: {doc.page_content[:100]}...")
```

---

## üí° Bonus-Aufgabe

**Aufgabe 6: Adaptive Chunk-Size basierend auf Sensitivit√§t**

```python
def adaptive_chunking(text: str, max_pii_density: float = 0.1) -> List[Dict]:
    """
    Passt Chunk-Size an, um PII-Dichte zu minimieren
    
    max_pii_density: Maximal 10% der Tokens d√ºrfen PII sein
    """
    from presidio_analyzer import AnalyzerEngine
    analyzer = AnalyzerEngine()
    
    # Start mit gro√üen Chunks
    chunk_size = 500
    while chunk_size >= 50:
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=20
        )
        
        chunks = splitter.split_text(text)
        
        # PII-Dichte pr√ºfen
        acceptable_chunks = 0
        
        for chunk in chunks:
            pii_results = analyzer.analyze(chunk, language="de")
            
            # Wie viel des Texts ist PII?
            pii_chars = sum(res.end - res.start for res in pii_results)
            pii_density = pii_chars / len(chunk) if len(chunk) > 0 else 0
            
            if pii_density <= max_pii_density:
                acceptable_chunks += 1
        
        acceptance_rate = acceptable_chunks / len(chunks) if chunks else 0
        
        print(f"Chunk-Size {chunk_size}: {acceptance_rate:.1%} akzeptabel")
        
        if acceptance_rate >= 0.8:  # 80% der Chunks sind ok
            print(f"‚Üí Optimale Chunk-Size gefunden: {chunk_size}")
            return chunks
        
        # Chunks verkleinern
        chunk_size -= 100
    
    print("‚ö†Ô∏è  Keine optimale Chunk-Size gefunden!")
    return []


# Test
optimal_chunks = adaptive_chunking(full_document, max_pii_density=0.15)
```

---

## üîç Reflexionsfragen

1. **Was ist besser: Viele kleine oder wenige gro√üe Chunks?**

2. **Wie beeinflusst Chunk-Overlap die Datensicherheit?**

3. **Sollten alle Chunks gleich gro√ü sein?**

4. **Kann man PII komplett aus Chunks entfernen?**

5. **Was ist der Trade-off zwischen Granularit√§t und Antwort-Qualit√§t?**

6. **Wie testet man, ob Chunking optimal ist?**
